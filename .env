APP_ENV=dev
DATA_DIR=artifacts/data
RUNS_DIR=artifacts/runs
MODELS_DIR=artifacts/models
CHROMA_DIR=artifacts/runs/chroma
LLM_ENDPOINT=http://localhost:8001/v1/chat/completions
LLM_MODEL=qwen2.5-7b-instruct
# LLM server settings for example_llm.py

# Where your LLM API is listening (local vLLM, LM Studio, or remote endpoint)
LLM_ENDPOINT=http://localhost:8001/v1/chat/completions

# Model name your endpoint expects (matches what your LLM server advertises)
LLM_MODEL=Qwen/Qwen2.5-7B-Instruct

# Only set if your endpoint requires authentication
# LLM_API_KEY=sk-...

# Optional: tuning defaults
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=600